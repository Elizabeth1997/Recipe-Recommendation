{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from DataSet import RecipeRecommendation\n",
    "from torch.utils.data import DataLoader\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"gpu\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe2vec = pd.read_csv('recipe2vec_1203.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>vector</th>\n",
       "      <th>indexes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['great northern beans', 'yellow onion', 'dice...</td>\n",
       "      <td>[0.04360911116666666, -0.005143107888888889, 0...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 7032, 7032, 7032, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"devil's food cake mix\", 'vegetable oil', 'eg...</td>\n",
       "      <td>[-0.17808505125000001, -0.18320775649999999, 0...</td>\n",
       "      <td>[9, 10, 11, 12, 7032, 7032, 7032, 7032, 7032, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>['mayonnaise', 'salsa', 'cheddar cheese', 'ref...</td>\n",
       "      <td>[-0.045958727461538465, 0.15744469853846155, 0...</td>\n",
       "      <td>[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>['chicken tenders', 'flour', 'garlic powder', ...</td>\n",
       "      <td>[0.010844033750000003, -0.11082101774999999, -...</td>\n",
       "      <td>[26, 27, 4, 28, 29, 30, 10, 31, 32, 33, 34, 35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>['lamb shoulder', 'salt', 'ground black pepper...</td>\n",
       "      <td>[0.026242995312500003, -0.16281941465, 0.00755...</td>\n",
       "      <td>[36, 28, 37, 10, 38, 39, 27, 40, 41, 42, 43, 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   i                                        ingredients  \\\n",
       "0  0  ['great northern beans', 'yellow onion', 'dice...   \n",
       "1  1  [\"devil's food cake mix\", 'vegetable oil', 'eg...   \n",
       "2  2  ['mayonnaise', 'salsa', 'cheddar cheese', 'ref...   \n",
       "3  3  ['chicken tenders', 'flour', 'garlic powder', ...   \n",
       "4  4  ['lamb shoulder', 'salt', 'ground black pepper...   \n",
       "\n",
       "                                              vector  \\\n",
       "0  [0.04360911116666666, -0.005143107888888889, 0...   \n",
       "1  [-0.17808505125000001, -0.18320775649999999, 0...   \n",
       "2  [-0.045958727461538465, 0.15744469853846155, 0...   \n",
       "3  [0.010844033750000003, -0.11082101774999999, -...   \n",
       "4  [0.026242995312500003, -0.16281941465, 0.00755...   \n",
       "\n",
       "                                             indexes  \n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 7032, 7032, 7032, ...  \n",
       "1  [9, 10, 11, 12, 7032, 7032, 7032, 7032, 7032, ...  \n",
       "2  [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 2...  \n",
       "3  [26, 27, 4, 28, 29, 30, 10, 31, 32, 33, 34, 35...  \n",
       "4  [36, 28, 37, 10, 38, 39, 27, 40, 41, 42, 43, 3...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe2vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe2vec_dict = recipe2vec.set_index('i')['indexes'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 7032, 7032, 7032, 7032, 7032, 7032, 7032, 7032, 7032, 7032, 7032]\n",
      "[9, 10, 11, 12, 7032, 7032, 7032, 7032, 7032, 7032, 7032, 7032, 7032, 7032, 7032, 7032, 7032, 7032, 7032, 7032]\n"
     ]
    }
   ],
   "source": [
    "# list_1 = json.loads(recipe2vec_dict[0])\n",
    "# list_2 = json.loads(recipe2vec_dict[1])\n",
    "# print(list_1)\n",
    "# print(list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = 'newdataset/interactions_train_1128.csv'\n",
    "# test becomes validation\n",
    "validation_data_path = 'newdataset/interactions_validation_1128.csv'\n",
    "whole_data_path = 'newdataset/whole_data_1128.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_negative_sample(whole_data_path):\n",
    "    whole_data = pd.read_csv(whole_data_path)\n",
    "    # tuple format\n",
    "    key = zip(whole_data['u'], whole_data['i'])\n",
    "    whole_data_dict = whole_data.set_index(key)['rating'].to_dict()\n",
    "    return whole_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = check_negative_sample(whole_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeNCFModel_v2(torch.nn.Module):\n",
    "    def __init__(self, u_emb_size, u_emb_dimension, r_emb_size, r_emb_dimension, pretrained_weight=None, ingre_embedding_tensors='Ingredient_embeddings_1202.pt'):\n",
    "        \n",
    "        super(RecipeNCFModel_v2, self).__init__()\n",
    "        self.u_emb_size = u_emb_size\n",
    "        self.u_emb_dimension = u_emb_dimension\n",
    "        self.r_emb_size = r_emb_size\n",
    "        self.r_emb_dimension = r_emb_dimension\n",
    "        self.input_size = self.u_emb_dimension + self.r_emb_dimension\n",
    "        \n",
    "        self.ingre_embedding_tensors = torch.load(ingre_embedding_tensors)\n",
    "        \n",
    "        self.pretrained_weight = pretrained_weight\n",
    "        \n",
    "        self.ingredient_embedding = nn.Embedding.from_pretrained(self.ingre_embedding_tensors) \n",
    "        \n",
    "        self.users_embedding = nn.Embedding(self.u_emb_size, self.u_emb_dimension)\n",
    "        self.recipe_embedding = nn.Embedding(self.r_emb_size, self.r_emb_dimension)\n",
    "        \n",
    "\n",
    "        self.hidden = nn.Sequential(nn.Linear(self.input_size, 16*2),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(16*2, 8*2),\n",
    "                                    nn.ReLU())\n",
    "#                                     nn.Linear(256, 128),\n",
    "#                                     nn.ReLU())\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Linear(6, 16*2),nn.ReLU())\n",
    "        \n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=2, kernel_size=(4, 100), stride=1, padding=0),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=2, kernel_size=(3, 100), stride=1, padding=0),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=2, kernel_size=(2, 100), stride=1, padding=0),\n",
    "            nn.ReLU())\n",
    "      \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.out = nn.Linear(8*2, 1)\n",
    "        \n",
    "        self._init()\n",
    "    \n",
    "    # initialize embedding layers and fc layers properly\n",
    "    def _init(self):\n",
    "        \n",
    "        def init(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "                \n",
    "        self.users_embedding.weight.data.uniform_(-0.05, 0.05)\n",
    "        if self.pretrained_weight is not None:\n",
    "            self.recipe_embedding.from_pretrained(self.pretrained_weight,freeze=False)\n",
    "            print('Use recipes embeddings successfully.')\n",
    "        else:\n",
    "            self.recipe_embedding.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.hidden.apply(init)\n",
    "        init(self.fc)\n",
    "        print('Weights have been initialized.')\n",
    "        \n",
    "    def forward(self, u, r):\n",
    "        # we should obtain the ingredients embeddings based on r whose data type is LongTensor\n",
    "        \n",
    "        u_emb = self.users_embedding(u)\n",
    "#         r_array = r.cpu().numpy().squeeze()\n",
    "        bs,_ = u.shape\n",
    "        # batch_size, 1, 20, 100: the shape of ingre\n",
    "        #  the number of channel is one\n",
    "        ingre = self.ingredient_embedding(r)\n",
    "        ingre = ingre.view(-1,1,20,100)\n",
    "        ingre_1 = self.cnn1(ingre)\n",
    "        ingre_1 = nn.MaxPool1d(ingre_1.shape[2])(ingre_1.view(-1,ingre_1.shape[1],ingre_1.shape[2]))\n",
    "        ingre_2 = self.cnn2(ingre)\n",
    "        ingre_2 = nn.MaxPool1d(ingre_2.shape[2])(ingre_2.view(-1,ingre_2.shape[1],ingre_2.shape[2]))\n",
    "        ingre_3 = self.cnn3(ingre)\n",
    "        ingre_3 = nn.MaxPool1d(ingre_3.shape[2])(ingre_3.view(-1,ingre_3.shape[1],ingre_3.shape[2]))\n",
    "        r_emb = self.recipe_embedding(r)\n",
    "        # 2 6 1\n",
    "        cat = torch.cat((ingre_1, ingre_2, ingre_3), dim = 1)\n",
    "        cat = self.fc(cat.view(-1,6))\n",
    "        x = torch.cat((u_emb, cat.view(-1,1,self.r_emb_dimension)), dim=2)\n",
    "        \n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.hidden(x)\n",
    "        out = self.out(x)\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_emb_size = 25076\n",
    "u_emb_dimension = 32\n",
    "r_emb_size = 178265\n",
    "r_emb_dimension = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights have been initialized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5073],\n",
       "        [0.5129]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RecipeNCFModel_v2(u_emb_size, u_emb_dimension, r_emb_size, r_emb_dimension)\n",
    "model(torch.LongTensor([[1],[2]]), torch.LongTensor([list_1,list_2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>load the pretrain model and print each layer's information</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Use the pretrain model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipe2vec = pd.read_csv('recipe2vec_1127.csv')\n",
    "# recipe2vec_array = recipe2vec['vector'].to_numpy()\n",
    "# recipe2vec_ = torch.Tensor([json.loads(each) for each in recipe2vec_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights have been initialized.\n"
     ]
    }
   ],
   "source": [
    "model = RecipeNCFModel_v2(u_emb_size, u_emb_dimension, r_emb_size, r_emb_dimension)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "learningRate = 1e-4\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learningRate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learningRate, momentum=0.9)\n",
    "epochs = 100000\n",
    "patience = 4\n",
    "no_improvements = 0\n",
    "best_ratio = -3\n",
    "best_weights = None\n",
    "training_loss_list = []\n",
    "validation_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580015 samples for training, 5312 samples for validation\n"
     ]
    }
   ],
   "source": [
    "train_dataset = RecipeRecommendation(training_data_path,scale=True)\n",
    "validation_dataset = RecipeRecommendation(validation_data_path,scale=True)\n",
    "train_data = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=train_dataset.collate)\n",
    "validation_data = DataLoader(validation_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=validation_dataset.collate)\n",
    "print('{} samples for training, {} samples for validation'.format(len(train_dataset), len(validation_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.empty(3, dtype=torch.long).random_(5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples_dict = {}\n",
    "hit_ratio_list = []\n",
    "negative_sample_ratio = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is started....\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e27e775a760f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training is started....'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# record the start time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "print('Training is started....')\n",
    "# record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_training_loss = []\n",
    "    \n",
    "    # name conflict!!! the index has the same name with recipe variable\n",
    "    for index_train, (u, r, rating) in enumerate(train_data):\n",
    "        \n",
    "        # obtain the batch size\n",
    "        bs, _ = u.shape\n",
    "        total_bs = bs * (1+negative_sample_ratio)\n",
    "        generated_batch = np.zeros((total_bs, 3))\n",
    "        \n",
    "        # convert tensor type to np.array\n",
    "        u_array = u.numpy().squeeze()\n",
    "        r_array = r.numpy().squeeze()\n",
    "        \n",
    "        # clear gradient\n",
    "        optimizer.zero_grad()\n",
    "        # sample negative (u,r) if ratio is one\n",
    "        #------------------------------------------------------#\n",
    "        negative_u = []\n",
    "        negative_r = []\n",
    "#         u_emb_size = 25076 [0,25075]\n",
    "#         r_emb_size = 178265 [0,178264]\n",
    "        # sample one negative for each positive interaction with the same user\n",
    "        for idx, each_user in enumerate(u_array):\n",
    "#             indexes = json.loads(recipe2vec_dict[r_array[idx]])\n",
    "            generated_batch[idx, :] = (each_user,r_array[idx], 1)\n",
    "        idx = idx + 1\n",
    "        \n",
    "        # produce neg samples\n",
    "        for each_user in u_array:\n",
    "            train_count = 0\n",
    "            while train_count < negative_sample_ratio:\n",
    "#                 Return a random integer N such that a <= N <= b\n",
    "                r_selected = random.randint(0, r_emb_size-1)\n",
    "                if (each_user, r_selected) in data_dict.keys():\n",
    "                    continue\n",
    "                else:\n",
    "#                     indexes = json.loads(recipe2vec_dict[r_selected])\n",
    "                    negative_samples_dict[(each_user,r_selected)] = 0\n",
    "                    negative_u.append(each_user)\n",
    "                    negative_r.append(r_selected)\n",
    "                    train_count = train_count + 1\n",
    "                    \n",
    "                    generated_batch[idx, :] = (each_user, r_selected, 0)\n",
    "                    idx = idx + 1\n",
    "        \n",
    "#         shuffle generated_batch obtained\n",
    "\n",
    "        np.random.shuffle(generated_batch)\n",
    "        indexes = []\n",
    "        each_list = generated_batch[:,1]\n",
    "        for each in each_list:\n",
    "            i_ = json.loads(recipe2vec_dict[int(each)])\n",
    "            indexes.append(i_)   \n",
    "        indexes_train = torch.LongTensor(indexes).to(device)\n",
    "\n",
    "#         ingre = get_ingre_embedding_tensors(generated_batch[:,1]).to(device)\n",
    "        u_train =  torch.LongTensor(generated_batch[:,0]).view(-1,1).to(device)\n",
    "#         r_train = torch.LongTensor(generated_batch[:,1]).view(-1,1).to(device)\n",
    "        label_train = torch.Tensor(generated_batch[:,2]).view(-1,1).to(device)\n",
    "        output = model(u_train,indexes_train)\n",
    "        \n",
    "        # calculate the loss\n",
    "        # CrossEntropyLoss\n",
    "        loss = criterion(output, label_train)\n",
    "        # doing backpropagation\n",
    "        loss.backward()\n",
    "        # get the current loss value\n",
    "        current_loss = loss.item()\n",
    "        if index_train % 50 == 0:\n",
    "            print(current_loss)\n",
    "        \n",
    "        epoch_training_loss.append(current_loss)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "      \n",
    "    \n",
    "    training_loss_list.append(statistics.mean(epoch_training_loss))\n",
    "    \n",
    "    print('epoch {}, training loss {:.4f}'.format(epoch, training_loss_list[epoch])) \n",
    "    \n",
    "    # evaluate\n",
    "    hit = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for index_validation, (u_validation, r_validation, rating_validation) in enumerate(validation_data):\n",
    "            va_negative_samples_dict = {}\n",
    "            u_validation_array = int(u_validation.numpy().squeeze())\n",
    "            r_validation_array = int(r_validation.numpy().squeeze())\n",
    "            positive_u_r = (u_validation_array, r_validation_array)\n",
    "            \n",
    "            r_validation = torch.LongTensor(json.loads(recipe2vec_dict[r_validation_array])).to(device)\n",
    "            \n",
    "#             ingre = get_ingre_embedding_tensors([u_validation.numpy().squeeze()]).to(device)\n",
    "            \n",
    "            u_validation = u_validation.to(device)\n",
    "#             r_validation = r_validation.to(device)\n",
    "            probability = model(u_validation,r_validation).cpu().numpy().squeeze()\n",
    "            probability = float(probability)\n",
    "       \n",
    "            negative_validation_u = []\n",
    "            negative_validation_r = []\n",
    "            \n",
    "            count = 0\n",
    "            while count < 100:\n",
    "                r_validation_selected = random.randint(0, r_emb_size-1)\n",
    "                sample = (u_validation_array, r_validation_selected)\n",
    "                if  sample in data_dict.keys():\n",
    "                    continue\n",
    "                else:\n",
    "                    va_negative_samples_dict[(u_validation_array,r_validation_selected)] = 0\n",
    "                    negative_validation_u.append(u_validation_array)\n",
    "                    negative_validation_r.append(r_validation_selected)\n",
    "                    count = count + 1\n",
    "                    \n",
    "            indexes = []\n",
    "            each_list = negative_validation_r\n",
    "            for each in each_list:\n",
    "                i_ = json.loads(recipe2vec_dict[int(each)])\n",
    "                indexes.append(i_)   \n",
    "            negative_r_val = torch.LongTensor(indexes).to(device)\n",
    "            \n",
    "#             ingre = get_ingre_embedding_tensors(np.array(negative_validation_r)).to(device)\n",
    "            negative_validation_u_array = np.array([negative_validation_u])\n",
    "            negative_validation_r_array = np.array([negative_validation_r])\n",
    "            \n",
    "            negative_u_val = torch.LongTensor(negative_validation_u_array).view(-1,1).to(device)\n",
    "#             negative_r_val = torch.LongTensor(negative_validation_r_array).view(-1,1).to(device)\n",
    "\n",
    "            # obtain all the probabilities of 100 negative samples\n",
    "            probabilities = model(negative_u_val,negative_r_val).cpu().numpy().squeeze()\n",
    "\n",
    "            key_ = zip(negative_validation_u_array.squeeze(), negative_validation_r_array.squeeze())\n",
    "            result_dict = dict(zip(key_, probabilities))\n",
    "            result_dict[positive_u_r] = probability\n",
    "            df = pd.DataFrame()\n",
    "            df['key'] = result_dict.keys()\n",
    "            df['value'] = result_dict.values()\n",
    "            result = df.sort_values(by='value', ascending=False)\n",
    "            # top 10\n",
    "            new_df = result.head(10)\n",
    "            find = new_df.loc[new_df['key']==positive_u_r]\n",
    "            positive_position = find.index\n",
    "            if len(find) != 0:\n",
    "                if positive_u_r in result_dict.keys():\n",
    "                    result_dict[positive_u_r] = result_dict[positive_u_r] + 1\n",
    "                else:\n",
    "                    result_dict[positive_u_r] = 1\n",
    "                hit = hit + 1\n",
    "#                 math.log(2) / math.log(positive_position+2)\n",
    "        print(hit, index_validation+1)\n",
    "        hit_ratio = hit / (index_validation+1)\n",
    "        hit_ratio_list.append(hit_ratio)\n",
    "        \n",
    "        if hit_ratio > best_ratio:\n",
    "            best_ratio = hit_ratio\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            no_improvements = 0\n",
    "        else:\n",
    "            no_improvements += 1\n",
    "        print('hit ratio: {:.3f}'.format(hit_ratio))\n",
    "        \n",
    "        if no_improvements >= patience:\n",
    "            break\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print('Total time {:.2f} min'.format(total_time / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMF : 10% HR\n",
    "<br>\n",
    "NCF: 13%HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H_%M_%S\", t)\n",
    "\n",
    "training_loss_df = pd.DataFrame(training_loss_list,columns=['training_loss'])\n",
    "training_loss_df.to_csv('result1202/train_loss_{}_1202.csv'.format(current_time), index=False)\n",
    "\n",
    "hit_ratio = pd.DataFrame(hit_ratio_list, columns=['hit_ratio'])\n",
    "hit_ratio.to_csv('result1202/hr_{}_1202.csv'.format(current_time), index=False)\n",
    "\n",
    "\n",
    "model_path = 'model1202/model_{}_1202.pth'.format(current_time)\n",
    "torch.save(best_weights, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
